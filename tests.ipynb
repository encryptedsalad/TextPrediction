{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss  : 4.317221  [    0/53426]\n",
      "recent: 4.317221  [    0/53426]\n",
      "sample output:  QF(01RrVZy.Cq\tFU1KcwSTxHFk.TR!qFB7Z:9\t?yMCZ\tYEks1Z93fzfBQJvAInJ+ neRq\t.pT)7SpvaGcsglvj!Q6fZpjWhBJTu8W\n",
      "loss  : 3.016701  [  100/53426]\n",
      "recent: 3.666961  [  100/53426]\n",
      "sample output:  \n",
      "Ub   e   u            t      t es  e     ae         .  he  s       eht                             \n",
      "loss  : 2.915500  [  200/53426]\n",
      "recent: 3.416474  [  200/53426]\n",
      "sample output:  Whe  o      o                                        t  he th t  oe the  oo                         \n",
      "loss  : 2.920746  [  300/53426]\n",
      "recent: 3.292542  [  300/53426]\n",
      "sample output:  re  ao     e t       te     aat    a                 he    ad  oot              a         an    e   \n",
      "loss  : 2.750640  [  400/53426]\n",
      "recent: 3.184162  [  400/53426]\n",
      "sample output:  \n",
      "    he                           hand mouthot  aou the  hou  oud the the  he the the the  r moo  he\n",
      "loss  : 2.508621  [  500/53426]\n",
      "recent: 3.071571  [  500/53426]\n",
      "sample output:  y wher wer  t ther whrt the  he    ourere the then   or the  hor  here th thed  hou ther the  he the\n",
      "loss  : 2.545667  [  600/53426]\n",
      "recent: 2.996442  [  600/53426]\n",
      "sample output:  l                                                                                                   \n",
      "loss  : 2.320017  [  700/53426]\n",
      "recent: 2.911889  [  700/53426]\n",
      "sample output:    o                                                                                                 \n",
      "loss  : 2.203215  [  800/53426]\n",
      "recent: 2.833148  [  800/53426]\n",
      "sample output:    he                                                                                                \n",
      "loss  : 3.601174  [  900/53426]\n",
      "recent: 2.909950  [  900/53426]\n",
      "sample output:  t           T  T I I I  WAR I I T ILENT I I I  T I I ITED TENANA I I I I  TELENA. I I I I  T I I I I\n",
      "loss  : 2.328690  [ 1000/53426]\n",
      "recent: 2.711097  [ 1000/53426]\n",
      "sample output:   him the thanger.\n",
      "    Aou  he have the  and the losthe the  he  he  he the  hathin the the the thand\n",
      "loss  : 2.003614  [ 1100/53426]\n",
      "recent: 2.609788  [ 1100/53426]\n",
      "sample output:   he  s t                                                                                            \n",
      "loss  : 2.153537  [ 1200/53426]\n",
      "recent: 2.533592  [ 1200/53426]\n",
      "sample output:    ar and har in the pare hat and he the the the the the cor the the the the cour the  ar and the war\n",
      "loss  : 2.397039  [ 1300/53426]\n",
      "recent: 2.481221  [ 1300/53426]\n",
      "sample output:    hat  are the coree  and the the the thee the ce and the be the gore me the thee  he  hame that  he\n",
      "loss  : 1.951135  [ 1400/53426]\n",
      "recent: 2.401271  [ 1400/53426]\n",
      "sample output:    his  hang an  he har  he bere thand and that her he thing the  for the wath me thith me the the  h\n",
      "loss  : 2.376177  [ 1500/53426]\n",
      "recent: 2.388027  [ 1500/53426]\n",
      "sample output:    he ser the s+   he sere the sond the  of the hather  he her and the sere the seathe  he her an  he \n",
      "loss  : 2.125333  [ 1600/53426]\n",
      "recent: 2.345993  [ 1600/53426]\n",
      "sample output:   wing an sere dore the site and are the sathe  and all the with  he sich and her the ther and tho  h\n",
      "loss  : 2.205368  [ 1700/53426]\n",
      "recent: 2.334528  [ 1700/53426]\n",
      "sample output:    he lathes and the  tore the ther thing the store t  have the then the  har serus  he her the thes \n",
      "loss  : 2.119544  [ 1800/53426]\n",
      "recent: 2.326161  [ 1800/53426]\n",
      "sample output:    a                                                                                                 \n",
      "loss  : 1.841988  [ 1900/53426]\n",
      "recent: 2.150243  [ 1900/53426]\n",
      "sample output:   he bed the which she her herer the firt he with  he her and bering+ \n",
      "    The hare the chene+  she her\n",
      "loss  : 1.170228  [ 2000/53426]\n",
      "recent: 2.034396  [ 2000/53426]\n",
      "sample output:   have the  hall I will be the thea mour the beand thaus  ha  hou hou hour mant the  hou have the  ho\n",
      "loss  : 2.070560  [ 2100/53426]\n",
      "recent: 2.041091  [ 2100/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 1.921930  [ 2200/53426]\n",
      "recent: 2.017930  [ 2200/53426]\n",
      "sample output:    he  he the sear  he the  he  he sere to the he the se sere the lound se were to  of rees for bee s\n",
      "loss  : 2.024626  [ 2300/53426]\n",
      "recent: 1.980689  [ 2300/53426]\n",
      "sample output:   the have he have an the  and have the  as  he have and and and the har ard ond have and the than th\n",
      "loss  : 2.040509  [ 2400/53426]\n",
      "recent: 1.989626  [ 2400/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 1.755781  [ 2500/53426]\n",
      "recent: 1.927587  [ 2500/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 2.001116  [ 2600/53426]\n",
      "recent: 1.915165  [ 2600/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 2.109316  [ 2700/53426]\n",
      "recent: 1.905560  [ 2700/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 2.052474  [ 2800/53426]\n",
      "recent: 1.898853  [ 2800/53426]\n",
      "sample output:                                                                                                      \n",
      "loss  : 1.715214  [ 2900/53426]\n",
      "recent: 1.886175  [ 2900/53426]\n",
      "sample output:   the sie the  hath mon shith me the will so me+  and the with me and  o                              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/all_shakespeare.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/shakespeare.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Programming/TextPrediction/RNN.py:77\u001b[0m, in \u001b[0;36mRNN.train_with_data\u001b[0;34m(self, trn_data_path, batch_size, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     75\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_num, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parts):\n\u001b[0;32m---> 77\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_to_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(states)\n\u001b[1;32m     79\u001b[0m     expected \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/Programming/TextPrediction/Tokens.py:21\u001b[0m, in \u001b[0;36mTokenizer.stream_to_states\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m     20\u001b[0m     out_list\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mtensor([token]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_unique_tokens)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from StackedGRU import StackedGRU\n",
    "from GRU import GRUModel\n",
    "from SimpleRNN import SimpleRNN\n",
    "import torch\n",
    "from Tokens import AlphabetTokenizer\n",
    "from Tokens import WordTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "alphabet_tokenizer = AlphabetTokenizer()\n",
    "word_tokenizer = WordTokenizer()\n",
    "\n",
    "model = SimpleRNN(word_tokenizer, 200).to(device)\n",
    "\n",
    "EPOCHS = 1\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train_with_data(\"data/all_shakespeare.txt\", 100)\n",
    "    \n",
    "torch.save(model.state_dict(), \"models/shakespeare.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
